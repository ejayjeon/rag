"""Streamlit web interface for Multimodal RAG"""
import streamlit as st
import tempfile
from pathlib import Path
import os
import time

# .env íŒŒì¼ ì§€ì› ì¶”ê°€
try:
    from dotenv import load_dotenv
    load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
except ImportError:
    pass  # python-dotenvê°€ ì—†ì–´ë„ ê³„ì† ì§„í–‰

# ì•ˆì „í•œ import with í´ë°±
try:
    from PIL import Image
except ImportError:
    Image = None

# src ëª¨ë“ˆ ì•ˆì „ import
try:
    from src.multimodal_rag import MultimodalRAG
    MULTIMODAL_RAG_AVAILABLE = True
except ImportError as e:
    MultimodalRAG = None
    MULTIMODAL_RAG_AVAILABLE = False
    IMPORT_ERROR = str(e)

# try:
from src.utils import (
    check_ollama_status, 
    get_ollama_models, 
    get_current_ollama_url, 
    test_ngrok_connection,
    update_ngrok_url,
    get_current_ngrok_url,
    call_ollama_api,
    generate_ollama_response
)
# except ImportError:
#     def check_ollama_status():
#         return False
#     def get_ollama_models():
#         return []


# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ RAG",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ RAG - í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•© ê²€ìƒ‰")

# í•¨ìˆ˜ ì •ì˜
def _verify_openai_key(api_key: str):
    """OpenAI API í‚¤ ê²€ì¦ í•¨ìˆ˜"""
    # API í‚¤ í˜•ì‹ ê²€ì¦
    if not api_key.startswith('sk-'):
        st.error("âŒ API í‚¤ëŠ” 'sk-'ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤")
        st.session_state.openai_verified = False
        return
    elif len(api_key) < 50:
        st.error("âŒ API í‚¤ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
        st.session_state.openai_verified = False
        return
    
    # ì‹¤ì‹œê°„ ì—°ê²° í…ŒìŠ¤íŠ¸
    with st.spinner("API í‚¤ ê²€ì¦ ì¤‘..."):
        try:
            import openai
            client = openai.OpenAI(api_key=api_key)
            models = client.models.list()
            st.success("âœ… OpenAI API í‚¤ ê²€ì¦ ì„±ê³µ!")
            st.session_state.openai_verified = True
        except openai.AuthenticationError:
            st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤")
            st.session_state.openai_verified = False
        except openai.PermissionDeniedError:
            st.error("âŒ API í‚¤ ê¶Œí•œì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
            st.session_state.openai_verified = False
        except openai.RateLimitError:
            st.warning("âš ï¸ ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”")
            st.session_state.openai_verified = False
        except Exception as e:
            st.error(f"âŒ ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            st.session_state.openai_verified = False

# ì˜ì¡´ì„± í™•ì¸
if not MULTIMODAL_RAG_AVAILABLE:
    st.error("âŒ MultimodalRAG ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.error(f"ì˜¤ë¥˜: {IMPORT_ERROR}")
    st.info("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
    st.code("pip install langchain langchain-community langchain-ollama chromadb sentence-transformers")
    st.stop()

# ì‚¬ì´ë“œë°” - ì‹œìŠ¤í…œ ìƒíƒœ
with st.sidebar:
    st.markdown("## ğŸ¤– LLM í”„ë¡œë°”ì´ë” ì„ íƒ")
    
    # ëª¨ë¸ ì„ íƒ
    llm_provider = st.selectbox(
        "LLM í”„ë¡œë°”ì´ë”",
        ["ollama", "openai"],
        help="OllamaëŠ” ë¬´ë£Œì´ì§€ë§Œ ë¡œì»¬ ì„¤ì¹˜ í•„ìš”, OpenAIëŠ” ìœ ë£Œì´ì§€ë§Œ ê³ í’ˆì§ˆ"
    )
    
    # OpenAI API Key ì…ë ¥ (OpenAI ì„ íƒì‹œì—ë§Œ í‘œì‹œ)
    if llm_provider == "openai":
        # API í‚¤ í™•ì¸ (ìš°ì„ ìˆœìœ„: Streamlit Secrets > í™˜ê²½ë³€ìˆ˜)
        auto_api_key = None
        key_source = None
        
        # 1. Streamlit Cloud Secrets í™•ì¸ (ë°°í¬ í™˜ê²½)
        try:
            if hasattr(st, 'secrets') and 'OPENAI_API_KEY' in st.secrets:
                auto_api_key = st.secrets["OPENAI_API_KEY"]
                key_source = "Streamlit Cloud Secrets"
        except Exception:
            pass
        
        # 2. í™˜ê²½ë³€ìˆ˜ í™•ì¸ (ë¡œì»¬ í™˜ê²½)
        if not auto_api_key:
            env_api_key = os.getenv('OPENAI_API_KEY')
            if env_api_key:
                auto_api_key = env_api_key
                key_source = "í™˜ê²½ë³€ìˆ˜"
        
        # API í‚¤ ì‚¬ìš© ë°©ë²• ì„ íƒ
        if auto_api_key:
            # í™˜ê²½ë³€ìˆ˜/Secretsì— í‚¤ê°€ ìˆëŠ” ê²½ìš°
            st.success(f"ğŸ”‘ {key_source}ì—ì„œ API í‚¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤ (sk-...{auto_api_key[-4:]})")
            
            api_key_mode = st.radio(
                "API í‚¤ ì‚¬ìš© ë°©ë²• ì„ íƒ:",
                ["ğŸ” ìë™ ì‚¬ìš© (ê¶Œì¥)", "âœï¸ ì§ì ‘ ì…ë ¥"],
                key="api_key_mode",
                help="ìë™ ì‚¬ìš©ì€ í™˜ê²½ë³€ìˆ˜ë‚˜ Secretsì˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ê³ , ì§ì ‘ ì…ë ¥ì€ ìƒˆë¡œìš´ í‚¤ë¥¼ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            if api_key_mode == "ğŸ” ìë™ ì‚¬ìš© (ê¶Œì¥)":
                # ìë™ í‚¤ ì‚¬ìš©
                st.session_state.openai_api_key = auto_api_key
                
                # í‚¤ ê²€ì¦
                with st.spinner(f"{key_source} API í‚¤ ê²€ì¦ ì¤‘..."):
                    try:
                        import openai
                        client = openai.OpenAI(api_key=auto_api_key)
                        models = client.models.list()
                        st.success(f"âœ… {key_source} API í‚¤ ê²€ì¦ ì„±ê³µ!")
                        st.session_state.openai_verified = True
                    except Exception as e:
                        st.error(f"âŒ {key_source} API í‚¤ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
                        st.session_state.openai_verified = False
            
            else:  # ì§ì ‘ ì…ë ¥ ëª¨ë“œ
                openai_api_key = st.text_input(
                    "OpenAI API Key (ì§ì ‘ ì…ë ¥)",
                    type="password",
                    help="ìƒˆë¡œìš´ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                    placeholder="sk-...",
                    key="manual_api_key"
                )
                
                if openai_api_key:
                    st.session_state.openai_api_key = openai_api_key
                    _verify_openai_key(openai_api_key)
                else:
                    st.session_state.openai_verified = False
        
        else:
            # í™˜ê²½ë³€ìˆ˜ì— í‚¤ê°€ ì—†ëŠ” ê²½ìš° - ì§ì ‘ ì…ë ¥ë§Œ ì œê³µ
            st.warning("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ë‚˜ Streamlit Secretsì— OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            st.info("ğŸ’¡ .env íŒŒì¼ì´ë‚˜ ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            openai_api_key = st.text_input(
                "OpenAI API Key",
                type="password",
                help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (sk-...)",
                placeholder="sk-...",
                key="fallback_api_key"
            )
            
            if openai_api_key:
                st.session_state.openai_api_key = openai_api_key
                _verify_openai_key(openai_api_key)
            else:
                st.session_state.openai_verified = False
    
    st.markdown("## ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ")
    
    # Ollama ìƒíƒœ í™•ì¸ (ollama ì„ íƒì‹œì—ë§Œ)
    if llm_provider == "ollama":
        try:
            # Streamlit Cloud í™˜ê²½ ê°ì§€
            import os
            is_streamlit_cloud = os.getenv('STREAMLIT_SERVER_PORT') is not None
            
            if is_streamlit_cloud:
                st.info("â˜ï¸ Streamlit Cloud í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘")
                st.warning("âš ï¸ Ollama ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” ngrok í„°ë„ì´ í•„ìš”í•©ë‹ˆë‹¤")
                
                # ngrok ì„¤ì • ì•ˆë‚´
                with st.expander("ğŸŒ ngrok ì„¤ì • ë°©ë²•", expanded=False):
                    st.markdown("""
                    **1. ë¡œì»¬ì—ì„œ ngrok ì‹¤í–‰:**
                    ```bash
                    ngrok http 11434
                    ```
                    
                    **2. ìƒì„±ëœ URLì„ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •:**
                    ```bash
                    export OLLAMA_HOST="https://xxxxx.ngrok-free.app"
                    ```
                    
                    **3. Streamlit Cloudì—ì„œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •:**
                    - Settings â†’ Secretsì— ì¶”ê°€:
                    ```toml
                    OLLAMA_HOST = "https://xxxxx.ngrok-free.app"
                    ```
                    """)
            
            ollama_status, current_url = check_ollama_status()
            if ollama_status:
                st.success("ğŸ¦™ Ollama ì—°ê²°ë¨")
                st.info(f"ğŸ“ ì—°ê²°ëœ ì„œë²„: {current_url}")
                
                # ngrok ì—°ê²° ìƒíƒœ í™•ì¸
                if "ngrok" in current_url:
                    st.success("ğŸŒ ngrok í„°ë„ ì—°ê²°ë¨")
                elif "localhost" in current_url:
                    st.info("ğŸ  ë¡œì»¬ ì„œë²„ ì—°ê²°ë¨")
            else:
                if is_streamlit_cloud:
                    st.error("âŒ ngrok í„°ë„ ì—°ê²° ì‹¤íŒ¨")
                    st.info("ğŸ’¡ ìœ„ì˜ ngrok ì„¤ì • ë°©ë²•ì„ ë”°ë¼ì£¼ì„¸ìš”")
                else:
                    st.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
                    st.info("Ollamaë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”:")
                    st.code("ollama serve")
                st.info("âš ï¸ OpenAIë¡œ ì „í™˜í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì²˜ë¦¬ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤")
        except Exception as e:
            st.error(f"âŒ Ollama ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
            st.info("âš ï¸ OpenAIë¡œ ì „í™˜í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ ë¬¸ì„œ ì²˜ë¦¬ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤")
    else:
        # OpenAI ì„ íƒì‹œ
        if st.session_state.get('openai_verified', False):
            st.success("ğŸ¤– OpenAI ì—°ê²° ì„±ê³µ!")
            
            # ì¶”ê°€ ì •ë³´ ë²„íŠ¼
            if st.button("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë³´ê¸°"):
                with st.spinner("ëª¨ë¸ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                    try:
                        import openai
                        client = openai.OpenAI(api_key=st.session_state.openai_api_key)
                        models = client.models.list()
                        
                        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í‘œì‹œ (ChatGPT ê´€ë ¨ ëª¨ë¸ ìš°ì„ )
                        chat_models = [m for m in models.data if any(x in m.id for x in ['gpt-', 'text-'])]
                        other_models = [m for m in models.data if m not in chat_models]
                        
                        with st.expander("ğŸ“‹ ì±„íŒ…/í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸", expanded=True):
                            for model in chat_models[:8]:
                                st.text(f"â€¢ {model.id}")
                        
                        if other_models:
                            with st.expander("ğŸ”§ ê¸°íƒ€ ëª¨ë¸"):
                                for model in other_models[:5]:
                                    st.text(f"â€¢ {model.id}")
                                    
                    except Exception as e:
                        st.error(f"âŒ ëª¨ë¸ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        
        elif 'openai_api_key' in st.session_state and st.session_state.openai_api_key:
            st.error("âŒ OpenAI API í‚¤ ê²€ì¦ ì‹¤íŒ¨")
            st.info("ğŸ’¡ ì˜¬ë°”ë¥¸ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        else:
            st.warning("ğŸ”‘ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    # # ngrok ì—°ê²° í…ŒìŠ¤íŠ¸ ë° URL ì—…ë°ì´íŠ¸
    # st.markdown("### ğŸŒ ngrok ì„¤ì •")
    
    # # í˜„ì¬ ngrok URL í‘œì‹œ
    # current_ngrok_url = get_current_ngrok_url()
    # st.info(f"í˜„ì¬ ngrok URL: `{current_ngrok_url}`")
    
    # # ngrok URL ì—…ë°ì´íŠ¸
    # new_ngrok_url = st.text_input(
    #     "ìƒˆë¡œìš´ ngrok URL ì…ë ¥",
    #     value=current_ngrok_url,
    #     help="ngrok URLì´ ë³€ê²½ë˜ì—ˆì„ ë•Œ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”"
    # )
    
    # if st.button("ğŸ”„ ngrok URL ì—…ë°ì´íŠ¸"):
    #     if new_ngrok_url != current_ngrok_url:
    #         update_ngrok_url(new_ngrok_url)
    #         st.success(f"ngrok URLì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤: {new_ngrok_url}")
    #         st.rerun()
    
    # # ngrok ì—°ê²° í…ŒìŠ¤íŠ¸ ë²„íŠ¼
    # if st.button("ğŸ” ngrok ì—°ê²° í…ŒìŠ¤íŠ¸"):
    #     with st.spinner("ngrok ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."):
    #         ngrok_success, ngrok_message = test_ngrok_connection()
    #         if ngrok_success:
    #             st.success(ngrok_message)
    #         else:
    #             st.error(ngrok_message)
    #             st.info("ğŸ’¡ ngrok URLì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì…ë ¥ì°½ì— ìƒˆë¡œìš´ URLì„ ì…ë ¥í•˜ì„¸ìš”.")
    
    # í”„ë¡œë°”ì´ë”ë³„ API í…ŒìŠ¤íŠ¸
    if llm_provider == "ollama":
        st.markdown("### ğŸ§ª Ollama API í…ŒìŠ¤íŠ¸")
        if st.button("ğŸ” API ì—°ê²° í…ŒìŠ¤íŠ¸"):
            with st.spinner("API ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."):
                try:
                    # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° í…ŒìŠ¤íŠ¸
                    models_response = call_ollama_api('api/tags')
                    if models_response and 'models' in models_response:
                        st.success(f"âœ… API ì—°ê²° ì„±ê³µ! ëª¨ë¸ {len(models_response['models'])}ê°œ ë°œê²¬")
                        # ëª¨ë¸ ëª©ë¡ í‘œì‹œ
                        with st.expander("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸"):
                            for model in models_response['models']:
                                model_name = model.get('name', 'Unknown')
                                st.text(f"â€¢ {model_name}")
                    else:
                        st.error("âŒ API ì—°ê²° ì‹¤íŒ¨")
                        st.info("ğŸ’¡ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
                except Exception as e:
                    st.error(f"âŒ API í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {str(e)}")
    elif llm_provider == "openai":
        st.markdown("### ğŸ¤– OpenAI API ìƒíƒœ")
        if st.session_state.get('openai_verified', False):
            st.success("âœ… OpenAI API ì—°ê²° í™•ì¸ë¨")
        else:
            st.warning("âš ï¸ ìœ„ì˜ API í‚¤ë¥¼ ì…ë ¥í•˜ì—¬ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”")
    
    # í”„ë¡œë°”ì´ë”ë³„ ëª¨ë¸ ìƒíƒœ í™•ì¸
    st.markdown("### ğŸ¤– ëª¨ë¸ ìƒíƒœ")
    
    if llm_provider == "ollama":
        models = get_ollama_models()
        try:
            ollama_status, _ = check_ollama_status()
            if ollama_status:
                # Vision ëª¨ë¸ (ì´ë¯¸ì§€ ë¶„ì„ìš©)
                if 'llava' in str(models).lower():
                    st.success("ğŸ‘ï¸ LLaVA ëª¨ë¸ ì¤€ë¹„ë¨ (ì´ë¯¸ì§€ ë¶„ì„ ê°€ëŠ¥)")
                else:
                    st.warning("âš ï¸ LLaVA ëª¨ë¸ ì—†ìŒ (ì´ë¯¸ì§€ ë¶„ì„ ë¶ˆê°€)")
                    st.code("ollama pull llava")
                
                # ì–¸ì–´ ëª¨ë¸ ê°ì§€ (í•œêµ­ì–´ ìš°ì„ )
                language_models = []
                for model in models:
                    if any(keyword in model.lower() for keyword in ['llama', 'gemma', 'qwen', 'mistral', 'code']):
                        language_models.append(model)
                
                if language_models:
                    st.success("ğŸ§  í…ìŠ¤íŠ¸ ì–¸ì–´ëª¨ë¸ ì¤€ë¹„ë¨")
                    # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì–¸ì–´ ëª¨ë¸ í‘œì‹œ
                    if 'current_language_model' in st.session_state:
                        current_model = st.session_state.current_language_model
                        if 'gemma2' in current_model.lower() or 'qwen' in current_model.lower():
                            st.success(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ìš°ì„ : {current_model}")
                        else:
                            st.info(f"í˜„ì¬ ì‚¬ìš©: {current_model}")
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ ëª¨ë¸ ëª©ë¡ í‘œì‹œ
                    with st.expander("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸"):
                        for model in language_models:
                            if 'gemma2' in model.lower() or 'qwen' in model.lower():
                                st.success(f"ğŸ‡°ğŸ‡· {model} (í•œêµ­ì–´ ìš°ìˆ˜)")
                            elif 'llama' in model.lower():
                                st.info(f"ğŸ¤– {model} (ì˜ì–´ ì¤‘ì‹¬)")
                            else:
                                st.info(f"ğŸŒ {model}")
                else:
                    st.warning("âš ï¸ ì–¸ì–´ëª¨ë¸ ì—†ìŒ")
                    st.code("ollama pull gemma2:9b")
                    st.info("ğŸ’¡ í•œêµ­ì–´ ì§€ì›ì„ ìœ„í•´ gemma2:9b ëª¨ë¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                st.warning("âš ï¸ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                st.info("ğŸ’¡ OpenAIë¡œ ì „í™˜í•˜ì‹œë©´ ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        except Exception as e:
            st.error(f"âŒ ëª¨ë¸ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
    
    elif llm_provider == "openai":
        if st.session_state.get('openai_verified', False):
            # OpenAI ëª¨ë¸ ìƒíƒœ ë° ì„ íƒ
            try:
                import openai
                client = openai.OpenAI(api_key=st.session_state.openai_api_key)
                available_models = client.models.list()
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í•„í„°ë§
                chat_models = [m.id for m in available_models.data if any(x in m.id for x in ['gpt-', 'text-'])]
                vision_models = [m.id for m in available_models.data if 'vision' in m.id.lower()]
                
                if chat_models:
                    st.success(f"ğŸ§  í…ìŠ¤íŠ¸ ëª¨ë¸: {len(chat_models)}ê°œ ì¤€ë¹„ë¨")
                    
                    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ í‘œì‹œ
                    if 'current_language_model' in st.session_state:
                        current_model = st.session_state.current_language_model
                        if 'gpt-4' in current_model:
                            st.success(f"ğŸš€ í˜„ì¬ ì„ íƒ: {current_model} (ìµœê³  í’ˆì§ˆ)")
                        elif 'gpt-3.5' in current_model:
                            st.info(f"âš¡ í˜„ì¬ ì„ íƒ: {current_model} (ë¹ ë¥¸ ì†ë„)")
                        else:
                            st.info(f"ğŸ”§ í˜„ì¬ ì„ íƒ: {current_model}")
                    
                    # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
                    selected_model = st.selectbox(
                        "ğŸ¤– í…ìŠ¤íŠ¸ ëª¨ë¸ ì„ íƒ",
                        options=chat_models,
                        index=0,
                        help="ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
                    )
                    
                    # ëª¨ë¸ë³„ íŠ¹ì§• ì•ˆë‚´
                    with st.expander("ğŸ“‹ ëª¨ë¸ë³„ íŠ¹ì§•", expanded=False):
                        for model in chat_models[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                            if 'gpt-4' in model:
                                st.success(f"ğŸš€ {model}: ìµœê³  í’ˆì§ˆ, ë†’ì€ ë¹„ìš©")
                            elif 'gpt-3.5' in model:
                                st.info(f"âš¡ {model}: ë¹ ë¥¸ ì†ë„, ì ë‹¹í•œ ë¹„ìš©")
                            else:
                                st.info(f"ğŸ”§ {model}: íŠ¹ìˆ˜ ìš©ë„")
                
                if vision_models:
                    st.success(f"ğŸ‘ï¸ Vision ëª¨ë¸: {len(vision_models)}ê°œ ì¤€ë¹„ë¨")
                    
                    # Vision ëª¨ë¸ ìë™ ì„ íƒ (ìµœì‹  ëª¨ë¸ ìš°ì„ )
                    recommended_vision_models = [
                        'gpt-4o',           # ìµœì‹  í†µí•© ëª¨ë¸
                        'gpt-4o-mini',      # ê²½ëŸ‰ ë²„ì „
                        'gpt-4-vision-preview',  # êµ¬ë²„ì „ (fallback)
                    ]
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ì—ì„œ ê¶Œì¥ ëª¨ë¸ ì°¾ê¸°
                    selected_vision_model = None
                    for recommended in recommended_vision_models:
                        if recommended in vision_models:
                            selected_vision_model = recommended
                            break
                    
                    if selected_vision_model:
                        st.success(f"ğŸ¯ ê¶Œì¥ Vision ëª¨ë¸: {selected_vision_model}")
                        # ì„¸ì…˜ì— Vision ëª¨ë¸ ì €ì¥
                        st.session_state.selected_vision_model = selected_vision_model
                    else:
                        st.warning("âš ï¸ ê¶Œì¥ Vision ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        selected_vision_model = vision_models[0] if vision_models else "gpt-4o"
                        st.info(f"ì‚¬ìš©í•  Vision ëª¨ë¸: {selected_vision_model}")
                    
                    with st.expander("ğŸ“‹ Vision ëª¨ë¸ ëª©ë¡", expanded=False):
                        for model in vision_models:
                            if model == selected_vision_model:
                                st.success(f"ğŸ‘ï¸ {model} (ì„ íƒë¨)")
                            else:
                                st.info(f"ğŸ‘ï¸ {model}")
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥ ì•ˆë‚´
                with st.expander("ğŸ“‹ OpenAI ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥", expanded=False):
                    st.info("â€¢ ğŸ¤– í…ìŠ¤íŠ¸ ìƒì„±: GPT-3.5/4 ì‹œë¦¬ì¦ˆ")
                    st.info("â€¢ ğŸ‘ï¸ ì´ë¯¸ì§€ ë¶„ì„: GPT-4o (ìµœì‹  Vision ëª¨ë¸)")
                    st.info("â€¢ ğŸ” ì´ë¯¸ì§€ OCR: í…ìŠ¤íŠ¸ ì¶”ì¶œ ê°€ëŠ¥")
                    st.info("â€¢ ğŸ“Š ì°¨íŠ¸/ê·¸ë˜í”„ ì´í•´: ë³µì¡í•œ ì‹œê° ë°ì´í„° ë¶„ì„")
                    st.info("â€¢ ğŸŒ ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´ í¬í•¨ 100+ ì–¸ì–´")
                    st.info("â€¢ âš¡ ë¹ ë¥¸ ì‘ë‹µì†ë„")
                    st.info("â€¢ ğŸ¯ ë†’ì€ í’ˆì§ˆ & ì •í™•ë„")
                    
            except Exception as e:
                st.warning(f"âš ï¸ ëª¨ë¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
                st.info("ğŸ’¡ API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”")
        else:
            st.warning("âš ï¸ OpenAI API í‚¤ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤")
            st.info("ğŸ’¡ ìœ„ì—ì„œ ìœ íš¨í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    st.markdown("---")
    st.markdown("## ğŸ“ ì§€ì› íŒŒì¼ í˜•ì‹")
    st.info("""
    â€¢ **í…ìŠ¤íŠ¸**: .txt, .md
    â€¢ **PDF**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì¶”ì¶œ
    â€¢ **ì´ë¯¸ì§€**: .jpg, .png, .gif ë“±
    """)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì„¹ì…˜
    st.markdown("---")
    st.markdown("### ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´")
    
    if llm_provider == "ollama":
        st.info("""
        **ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ:**
        â€¢ LangChain + ChromaDB
        â€¢ Sentence Transformers
        â€¢ Ollama (ë¡œì»¬ LLM)
        """)
        
        st.info("""
        **ğŸ’¡ íŒ:**
        â€¢ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œ ê°€ëŠ¥
        â€¢ Ollama ì„œë²„ ì‹¤í–‰ í•„ìš”
        â€¢ ì´ë¯¸ì§€ ë¶„ì„ì€ LLaVA ëª¨ë¸ í•„ìš”
        """)
    else:
        st.info("""
        **ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ:**
        â€¢ LangChain + ChromaDB
        â€¢ Sentence Transformers
        â€¢ OpenAI API (í´ë¼ìš°ë“œ LLM)
        """)
        
        st.info("""
        **ğŸ’¡ íŒ:**
        â€¢ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œ ê°€ëŠ¥
        â€¢ ì¸í„°ë„· ì—°ê²° í•„ìš”
        â€¢ API ì‚¬ìš©ë£Œ ë°œìƒ (í† í°ë‹¹ ê³¼ê¸ˆ)
        """)

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì¡°ê±´ë¶€)
should_initialize = False
if llm_provider == "ollama":
    should_initialize = True  # OllamaëŠ” í•­ìƒ ì´ˆê¸°í™” ê°€ëŠ¥
elif llm_provider == "openai":
    # OpenAIëŠ” í‚¤ê°€ ìˆê±°ë‚˜ ê²€ì¦ëœ ê²½ìš°ë§Œ ì´ˆê¸°í™”
    env_key = os.getenv('OPENAI_API_KEY')
    has_env_key = bool(env_key)
    has_verified_key = st.session_state.get('openai_verified', False)
    should_initialize = has_env_key or has_verified_key

if 'rag_system' not in st.session_state and should_initialize:
    try:
        with st.spinner("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            language_model = None
            
            if llm_provider == "ollama":
                # Ollama: ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ ëª¨ë¸ ìë™ ê°ì§€
                available_models = get_ollama_models()
                
                # í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜ ëª¨ë¸ ìš°ì„  ì„ íƒ (ìš°ì„ ìˆœìœ„ ìˆœ)
                korean_priority_models = [
                    'gemma2:9b',      # í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜
                    'qwen2.5:7b',     # í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜
                    'codellama:7b',   # ë‹¤êµ­ì–´ ì§€ì›
                    'mistral:7b',     # ë‹¤êµ­ì–´ ì§€ì›
                ]
                
                # ìš°ì„ ìˆœìœ„ ëª¨ë¸ ì¤‘ì—ì„œ ì°¾ê¸°
                for priority_model in korean_priority_models:
                    if any(priority_model.split(':')[0] in model.lower() for model in available_models):
                        language_model = priority_model
                        break
                
                # ìš°ì„ ìˆœìœ„ ëª¨ë¸ì´ ì—†ìœ¼ë©´ llama ê³„ì—´ ëª¨ë¸ ì°¾ê¸°
                if not language_model:
                    for model in available_models:
                        if 'llama' in model.lower():
                            language_model = model
                            break
                
                if not language_model:
                    # ê¸°ë³¸ê°’ ì„¤ì •
                    language_model = "gemma2:9b"
                    st.warning(f"âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì–¸ì–´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ '{language_model}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    st.info("ğŸ’¡ í•œêµ­ì–´ ì§€ì›ì„ ìœ„í•´ gemma2:9b ëª¨ë¸ì„ ì„¤ì¹˜í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
                else:
                    if 'gemma2' in language_model.lower() or 'qwen' in language_model.lower():
                        st.success(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ìš°ì„  ëª¨ë¸ '{language_model}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!")
                    else:
                        st.info(f"ğŸ¤– ì–¸ì–´ ëª¨ë¸ '{language_model}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
            # OpenAI API í‚¤ ì„¤ì • (ìš°ì„ ìˆœìœ„: ê²€ì¦ëœ í‚¤ > í™˜ê²½ë³€ìˆ˜)
            openai_api_key = None
            if llm_provider == "openai":
                if st.session_state.get('openai_verified', False) and st.session_state.get('openai_api_key'):
                    openai_api_key = st.session_state.openai_api_key
                else:
                    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
                    env_key = os.getenv('OPENAI_API_KEY')
                    if env_key:
                        openai_api_key = env_key
                        st.info("ğŸ”‘ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
                    else:
                        st.warning("â³ OpenAI API í‚¤ ì„¤ì •ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                        st.info("ğŸ’¡ ìœ„ì˜ ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
                        st.stop()
            
            elif llm_provider == "openai":
                # OpenAI: ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¤‘ì—ì„œ ì„ íƒ
                try:
                    import openai
                    client = openai.OpenAI(api_key=openai_api_key)
                    available_models = client.models.list()
                    
                    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í•„í„°ë§ (ChatGPT ê´€ë ¨ ëª¨ë¸ ìš°ì„ )
                    chat_models = [m.id for m in available_models.data if any(x in m.id for x in ['gpt-', 'text-'])]
                    
                    if chat_models:
                        # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
                        selected_model = st.selectbox(
                            "ğŸ¤– OpenAI ëª¨ë¸ ì„ íƒ",
                            options=chat_models,
                            index=0,  # ê¸°ë³¸ê°’: ì²« ë²ˆì§¸ ëª¨ë¸
                            help="ì‚¬ìš©í•  OpenAI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. GPT-4ëŠ” ë” ì •í™•í•˜ì§€ë§Œ ë¹„ìš©ì´ ë†’ìŠµë‹ˆë‹¤."
                        )
                        language_model = selected_model
                        st.success(f"ğŸ¤– OpenAI ëª¨ë¸ '{language_model}'ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤!")
                        
                        # ëª¨ë¸ë³„ íŠ¹ì§• ì•ˆë‚´
                        if 'gpt-4' in language_model:
                            st.info("ğŸš€ GPT-4: ìµœê³  í’ˆì§ˆ, ë†’ì€ ë¹„ìš©")
                        elif 'gpt-3.5' in language_model:
                            st.info("âš¡ GPT-3.5: ë¹ ë¥¸ ì†ë„, ì ë‹¹í•œ ë¹„ìš©")
                        else:
                            st.info("ğŸ”§ ê¸°íƒ€ ëª¨ë¸: íŠ¹ìˆ˜ ìš©ë„")
                    else:
                        # ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
                        language_model = "gpt-3.5-turbo"
                        st.warning(f"âš ï¸ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ '{language_model}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        
                except Exception as e:
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                    language_model = "gpt-3.5-turbo"
                    st.warning(f"âš ï¸ ëª¨ë¸ ì„ íƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.info(f"ê¸°ë³¸ê°’ '{language_model}'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                # ì„ íƒëœ ëª¨ë¸ì„ ì„¸ì…˜ì— ì €ì¥
                if 'selected_model' in locals():
                    st.session_state.selected_openai_model = selected_model
            
            # Vision ëª¨ë¸ ì„ íƒ (OpenAIì¸ ê²½ìš°)
            if llm_provider == "openai":
                # ì„¸ì…˜ì—ì„œ ì„ íƒëœ Vision ëª¨ë¸ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
                vision_model = st.session_state.get('selected_vision_model', 'gpt-4o')
                st.info(f"ğŸ‘ï¸ Vision ëª¨ë¸: {vision_model} ì‚¬ìš©")
            else:
                vision_model = "llava"
            
            st.session_state.rag_system = MultimodalRAG(
                llm_model=language_model,
                vision_model=vision_model,
                llm_provider=llm_provider,
                openai_api_key=openai_api_key
            )
            st.session_state.documents_added = False
            st.session_state.current_language_model = language_model
            
        st.success("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        st.stop()

# ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì€ ê²½ìš° ë©”ì‹œì§€ í‘œì‹œ
if 'rag_system' not in st.session_state:
    if llm_provider == "openai":
        st.info("ğŸ”‘ OpenAI API í‚¤ ì„¤ì • í›„ ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤")
        st.info("ğŸ‘† ìœ„ì˜ ì‚¬ì´ë“œë°”ì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
    else:
        st.info("âš™ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë¨¼ì € ì‹¤í–‰)
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = []

if 'file_contents' not in st.session_state:
    st.session_state.file_contents = {}

if 'conversation_context' not in st.session_state:
    st.session_state.conversation_context = {
        'uploaded_files': [],
        'file_summaries': {},
        'last_query': None,
        'session_start': time.time()
    }

# ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
if st.session_state.uploaded_files:
    st.markdown("### ğŸ§  ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸")
    with st.expander("ğŸ“Š í˜„ì¬ ìƒíƒœ", expanded=False):
        context = st.session_state.conversation_context
        st.info(f"â° ì„¸ì…˜ ì‹œì‘: {time.strftime('%H:%M:%S', time.localtime(context['session_start']))}")
        st.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼: {len(st.session_state.uploaded_files)}ê°œ")
        st.info(f"ğŸ’¬ ëŒ€í™” ìˆ˜: {len(st.session_state.chat_history)}ê°œ")
        
        if context['last_query']:
            st.info(f"ğŸ” ë§ˆì§€ë§‰ ì§ˆë¬¸: {context['last_query'][:50]}...")
        
        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”„ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”", type="secondary"):
            st.session_state.conversation_context = {
                'uploaded_files': [],
                'file_summaries': {},
                'last_query': None,
                'session_start': time.time()
            }
            st.session_state.chat_history = []
            st.success("âœ… ì»¨í…ìŠ¤íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

# íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
st.markdown("## ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")

uploaded_files = st.file_uploader(
    "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)",
    type=['txt', 'md', 'pdf', 'jpg', 'jpeg', 'png', 'gif', 'bmp'],
    accept_multiple_files=True,
    help="í…ìŠ¤íŠ¸, PDF, ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
)

# íŒŒì¼ ì²˜ë¦¬
if uploaded_files and st.button("ğŸ“š ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘", type="primary"):
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    successful_files = []
    failed_files = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = (i + 1) / len(uploaded_files)
            progress_bar.progress(progress)
            status_text.text(f"ì²˜ë¦¬ ì¤‘: {uploaded_file.name} ({i+1}/{len(uploaded_files)})")
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            suffix = Path(uploaded_file.name).suffix
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                temp_path = tmp_file.name
            
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
            if suffix.lower() == '.pdf':
                st.session_state.rag_system.add_pdf_document(temp_path)
            elif suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp']:
                st.session_state.rag_system.add_image_document(temp_path)
            elif suffix.lower() in ['.txt', '.md']:
                st.session_state.rag_system.add_text_document(temp_path)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {suffix}")
            
            # íŒŒì¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            file_info = {
                'name': uploaded_file.name,
                'type': suffix.lower(),
                'size': len(uploaded_file.getvalue()),
                'upload_time': time.time(),
                'processed': True
            }
            st.session_state.uploaded_files.append(file_info)
            
            # íŒŒì¼ ë‚´ìš© ìš”ì•½ ì €ì¥ (í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ê²½ìš°)
            if suffix.lower() in ['.txt', '.md']:
                try:
                    with open(temp_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì²« 200ì)
                        summary = content[:200] + "..." if len(content) > 200 else content
                        st.session_state.file_contents[uploaded_file.name] = {
                            'type': 'text',
                            'summary': summary,
                            'full_content': content
                        }
                except Exception as e:
                    st.warning(f"íŒŒì¼ ë‚´ìš© ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            
            successful_files.append(uploaded_file.name)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(temp_path)
            
        except Exception as e:
            failed_files.append((uploaded_file.name, str(e)))
            if 'temp_path' in locals():
                try:
                    os.unlink(temp_path)
                except:
                    pass
    
    # ì²˜ë¦¬ ê²°ê³¼
    progress_bar.empty()
    status_text.empty()
    
    if successful_files:
        st.session_state.documents_added = True
        st.success(f"âœ… {len(successful_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        
        with st.expander("ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡"):
            for filename in successful_files:
                st.write(f"âœ… {filename}")
    
    if failed_files:
        st.error(f"âŒ {len(failed_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨")
        with st.expander("ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡"):
            for filename, error in failed_files:
                st.write(f"âŒ {filename}: {error}")
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ
    if hasattr(st.session_state.rag_system, 'get_status'):
        status = st.session_state.rag_system.get_status()
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ğŸ“„ í…ìŠ¤íŠ¸ ë¬¸ì„œ", status.get('text_documents', 0))
        with col2:
            st.metric("ğŸ–¼ï¸ ì´ë¯¸ì§€", status.get('image_documents', 0))
        with col3:
            st.metric("ğŸ§  ì´ ë¬¸ì„œ", status.get('total_documents', 0))
    
    # íŒŒì¼ íˆìŠ¤í† ë¦¬ í‘œì‹œ
    if st.session_state.uploaded_files:
        st.markdown("### ğŸ“š ì—…ë¡œë“œëœ íŒŒì¼ íˆìŠ¤í† ë¦¬")
        with st.expander("ğŸ“‹ íŒŒì¼ ëª©ë¡ ë° ìš”ì•½", expanded=True):
            for i, file_info in enumerate(st.session_state.uploaded_files):
                col1, col2, col3 = st.columns([2, 1, 1])
                
                with col1:
                    file_icon = "ğŸ“„" if file_info['type'] in ['.txt', '.md'] else "ğŸ–¼ï¸" if file_info['type'] in ['.jpg', '.jpeg', '.png', '.gif', '.bmp'] else "ğŸ“•"
                    st.write(f"{file_icon} **{file_info['name']}**")
                
                with col2:
                    st.write(f"**íƒ€ì…**: {file_info['type']}")
                
                with col3:
                    size_kb = file_info['size'] / 1024
                    st.write(f"**í¬ê¸°**: {size_kb:.1f} KB")
                
                # íŒŒì¼ ë‚´ìš© ìš”ì•½ í‘œì‹œ (í…ìŠ¤íŠ¸ íŒŒì¼ì˜ ê²½ìš°)
                if file_info['name'] in st.session_state.file_contents:
                    content_info = st.session_state.file_contents[file_info['name']]
                    if content_info['type'] == 'text':
                        st.text_area(
                            f"ğŸ“ {file_info['name']} ë‚´ìš© ìš”ì•½",
                            content_info['summary'],
                            height=80,
                            key=f"summary_{i}",
                            disabled=True
                        )
                
                st.markdown("---")

# ì„¸ì…˜ ìƒíƒœëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì´ˆê¸°í™”ë¨

# ì§ˆë¬¸ ì„¹ì…˜
if st.session_state.get('documents_added', False):
    st.markdown("---")
    st.markdown("## ğŸ’¬ ì±„íŒ…í˜• ì§ˆë¬¸í•˜ê¸°")
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.chat_history:
                    if message['role'] == 'user':
                        with st.chat_message("user"):
                            st.write(message['content'])
                            
                            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ (ì‚¬ìš©ì ë©”ì‹œì§€)
                            if 'context' in message:
                                with st.expander("ğŸ” ì»¨í…ìŠ¤íŠ¸ ì •ë³´", expanded=False):
                                    context = message['context']
                                    st.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼: {context['uploaded_files']}ê°œ")
                                    st.info(f"ğŸ“„ íŒŒì¼ íƒ€ì…: {', '.join(context['file_types'])}")
                                    st.info(f"â±ï¸ ì„¸ì…˜ ì‹œê°„: {context['session_duration']:.1f}ì´ˆ")
                    else:
                        with st.chat_message("assistant"):
                            # ì˜¤ë¥˜ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
                            if message.get('error', False):
                                st.error("âŒ ì˜¤ë¥˜ ë°œìƒ")
                                st.write(message['content'])
                                
                                # ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ í‘œì‹œ
                                if 'error_details' in message:
                                    with st.expander("ğŸ” ì˜¤ë¥˜ ìƒì„¸ ì •ë³´", expanded=False):
                                        st.code(message['error_details'])
                                        
                                        # ì˜¤ë¥˜ í•´ê²° ë°©ë²• ì œì•ˆ
                                        st.markdown("**ğŸ’¡ í•´ê²° ë°©ë²•:**")
                                        st.markdown("â€¢ OpenAI API í‚¤ í™•ì¸")
                                        st.markdown("â€¢ ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸")
                                        st.markdown("â€¢ API ì‚¬ìš©ëŸ‰ í•œë„ í™•ì¸")
                                        st.markdown("â€¢ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„")
                            else:
                                st.write(message['content'])
                                
                                # ë©”íƒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                                if 'confidence' in message:
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.metric("ğŸ¯ ì‹ ë¢°ë„", f"{message['confidence']:.1%}")
                                    with col2:
                                        st.metric("ğŸ“š ì°¸ì¡° ì†ŒìŠ¤", message.get('source_count', 0))
                                
                                # ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš© ì •ë³´ í‘œì‹œ
                                if 'context_used' in message:
                                    context_used = message['context_used']
                                    st.success(f"âœ… ì»¨í…ìŠ¤íŠ¸ ì¸ì‹: {context_used['files_referenced']}ê°œ íŒŒì¼ ì°¸ì¡°ë¨")
                                
                                # ì°¸ì¡° ì†ŒìŠ¤ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                                if 'sources' in message and message['sources']:
                                    with st.expander("ğŸ“š ì°¸ì¡°ëœ ì†ŒìŠ¤ ë³´ê¸°"):
                                        for i, doc in enumerate(message['sources']):
                                            st.markdown(f"**ì†ŒìŠ¤ {i+1}:**")
                                            
                                            # ë©”íƒ€ë°ì´í„°
                                            metadata = doc.metadata
                                            source_type = metadata.get('type', 'ì•Œ ìˆ˜ ì—†ìŒ')
                                            source_name = metadata.get('filename', metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ'))
                                            
                                            st.write(f"ğŸ“„ **ìœ í˜•**: {source_type}")
                                            st.write(f"ğŸ“ **íŒŒì¼**: {Path(source_name).name}")
                                            
                                            # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                                            content = doc.page_content[:300]
                                            if len(doc.page_content) > 300:
                                                content += "..."
                                            
                                            st.text_area(
                                                f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° {i+1}",
                                                content,
                                                height=100,
                                                key=f"source_{i}_{message.get('timestamp', 0)}",
                                                disabled=True
                                            )
                                            st.markdown("---")
    
    # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼
    # st.markdown("**ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸:**")
    # col1, col2, col3 = st.columns(3)
    
    # with col1:
    #     if st.button("ğŸ“Š ë¬¸ì„œ ìš”ì•½", key="summary_btn"):
    #         st.session_state.example_query = "ì—…ë¡œë“œëœ ëª¨ë“  ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”."
    
    # with col2:
    #     if st.button("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„¤ëª…", key="image_btn"):
    #         st.session_state.example_query = "ì´ë¯¸ì§€ì—ì„œ ë°œê²¬ë˜ëŠ” ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    
    # with col3:
    #     if st.button("ğŸ” ìƒì„¸ ë¶„ì„", key="analysis_btn"):
    #         st.session_state.example_query = "ë¬¸ì„œì™€ ì´ë¯¸ì§€ë¥¼ ì¢…í•©í•˜ì—¬ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”."
    
    # ìƒˆë¡œìš´ ì§ˆë¬¸ ì…ë ¥ (ì±„íŒ… í˜•íƒœ)
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì—…ë°ì´íŠ¸
        st.session_state.conversation_context['last_query'] = prompt
        
        # ì‚¬ìš©ì ì§ˆë¬¸ì„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        user_message = {
            'role': 'user',
            'content': prompt,
            'timestamp': len(st.session_state.chat_history),
            'context': {
                'uploaded_files': len(st.session_state.uploaded_files),
                'file_types': [f['type'] for f in st.session_state.uploaded_files],
                'session_duration': time.time() - st.session_state.conversation_context['session_start']
            }
        }
        st.session_state.chat_history.append(user_message)
        
        # ë‹µë³€ ìƒì„±
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì§ˆë¬¸ ìƒì„±
                context_prompt = f"""
                ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
                - ì—…ë¡œë“œëœ íŒŒì¼ ìˆ˜: {len(st.session_state.uploaded_files)}
                - íŒŒì¼ íƒ€ì…: {', '.join([f['type'] for f in st.session_state.uploaded_files])}
                - ì´ì „ ëŒ€í™” ìˆ˜: {len(st.session_state.chat_history) - 1}
                
                ì‚¬ìš©ì ì§ˆë¬¸: {prompt}
                
                ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”. ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì´ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
                """
                
                # OpenAI ì‚¬ìš© ì‹œ íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§
                max_retries = 3
                retry_count = 0
                
                while retry_count < max_retries:
                    try:
                        result = st.session_state.rag_system.search(context_prompt)
                        break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                    except Exception as retry_error:
                        retry_count += 1
                        if retry_count >= max_retries:
                            # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                            error_msg = f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {max_retries}íšŒ): {str(retry_error)}"
                            st.error(error_msg)
                            
                            # ì˜¤ë¥˜ ì •ë³´ë¥¼ ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                            error_message = {
                                'role': 'assistant',
                                'content': error_msg,
                                'timestamp': len(st.session_state.chat_history),
                                'error': True,
                                'error_details': str(retry_error)
                            }
                            st.session_state.chat_history.append(error_message)
                            st.rerun()
                            break  # while ë£¨í”„ íƒˆì¶œ
                        else:
                            # ì¬ì‹œë„ ì•ˆë‚´
                            st.warning(f"âš ï¸ API í˜¸ì¶œ ì‹¤íŒ¨, {retry_count}/{max_retries} ì¬ì‹œë„ ì¤‘...")
                            time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                
                # AI ë‹µë³€ì„ ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                assistant_message = {
                    'role': 'assistant',
                    'content': result.answer,
                    'confidence': result.confidence,
                    'source_count': len(result.sources),
                    'sources': result.sources,
                    'timestamp': len(st.session_state.chat_history),
                    'context_used': {
                        'files_referenced': len(result.sources),
                        'query_understood': True
                    }
                }
                st.session_state.chat_history.append(assistant_message)
                
                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ìµœì‹  ëŒ€í™” í‘œì‹œ
                st.rerun()
                
            except Exception as e:
                st.error(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ì˜ˆì‹œ ì¿¼ë¦¬ ì´ˆê¸°í™”
        if 'example_query' in st.session_state:
            del st.session_state.example_query
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë²„íŠ¼
    if st.session_state.chat_history:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°", key="clear_chat"):
            st.session_state.chat_history = []
            st.rerun()

else:
    st.info("ğŸ‘† íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•œ í›„ì— ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì‚¬ìš© ê°€ì´ë“œ
    st.markdown("### ğŸ¯ ì‚¬ìš© ê°€ì´ë“œ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **ğŸ“„ ì§€ì› íŒŒì¼:**
        â€¢ í…ìŠ¤íŠ¸ íŒŒì¼ (.txt, .md)
        â€¢ PDF ë¬¸ì„œ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        â€¢ ì´ë¯¸ì§€ íŒŒì¼ (jpg, png ë“±)
        """)
    
    with col2:
        if llm_provider == "ollama":
            st.markdown("""
            **ğŸ¤– ê¸°ëŠ¥:**
            â€¢ í…ìŠ¤íŠ¸ ë¬¸ì„œ ê²€ìƒ‰
            â€¢ ì´ë¯¸ì§€ ë‚´ìš© ë¶„ì„ (LLaVA ëª¨ë¸ í•„ìš”)
            â€¢ ë©€í‹°ëª¨ë‹¬ í†µí•© ê²€ìƒ‰
            """)
        else:
            st.markdown("""
            **ğŸ¤– ê¸°ëŠ¥:**
            â€¢ í…ìŠ¤íŠ¸ ë¬¸ì„œ ê²€ìƒ‰
            â€¢ ì´ë¯¸ì§€ ë‚´ìš© ë¶„ì„ (GPT-4o Vision)
            â€¢ ë©€í‹°ëª¨ë‹¬ í†µí•© ê²€ìƒ‰
            """)

# í•˜ë‹¨ ì •ë³´ëŠ” ì‚¬ì´ë“œë°”ë¡œ ì´ë™ë¨