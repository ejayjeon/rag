# ================================
# 3. ì„¤ì • ê´€ë¦¬ (src/core/config.py)
# ================================

import os
from pathlib import Path
from enum import Enum
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class LLMProvider(Enum):
    """LLM ì œê³µì ì—´ê±°í˜•"""
    OLLAMA = "ollama"
    OPENAI = "openai"

class Config:
    """í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬"""
    
    # ê¸°ë³¸ ê²½ë¡œ
    BASE_DIR = Path(__file__).parent.parent.parent
    MODELS_DIR = BASE_DIR / "models"
    TEMP_DIR = BASE_DIR / "temp"
    
    # Whisper ì„¤ì •
    WHISPER_MODEL = os.getenv("WHISPER_MODEL", "base")
    WHISPER_LANGUAGE = os.getenv("WHISPER_LANGUAGE", "ko")
    
    # LLM Provider ì„¤ì •
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "ollama").lower()
    
    # Ollama ì„¤ì •
    OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama2")
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    # OpenAI ì„¤ì •
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "")  # ì»¤ìŠ¤í…€ ì—”ë“œí¬ì¸íŠ¸ìš©
    
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ ì„¤ì •
    LLM_MODEL = os.getenv("LLM_MODEL", OLLAMA_MODEL)
    LLM_BASE_URL = os.getenv("LLM_BASE_URL", OLLAMA_BASE_URL)
    
    # ì²˜ë¦¬ ì„¤ì •
    MAX_AUDIO_SIZE_MB = int(os.getenv("MAX_AUDIO_SIZE_MB", "50"))
    SESSION_TIMEOUT_MINUTES = int(os.getenv("SESSION_TIMEOUT_MINUTES", "30"))
    
    # API ì„¤ì • (Phase 2ìš©)
    API_HOST = os.getenv("API_HOST", "0.0.0.0")
    API_PORT = int(os.getenv("API_PORT", "8000"))
    
    @classmethod
    def get_current_llm_provider(cls) -> LLMProvider:
        """í˜„ì¬ ì„¤ì •ëœ LLM ì œê³µì ë°˜í™˜"""
        if cls.LLM_PROVIDER == "openai":
            return LLMProvider.OPENAI
        return LLMProvider.OLLAMA
    
    @classmethod
    def get_llm_config(cls):
        """í˜„ì¬ LLM ì œê³µìì— ë”°ë¥¸ ì„¤ì • ë°˜í™˜"""
        provider = cls.get_current_llm_provider()
        
        if provider == LLMProvider.OPENAI:
            return {
                "provider": "openai",
                "model": cls.OPENAI_MODEL,
                "api_key": cls.OPENAI_API_KEY,
                "base_url": cls.OPENAI_BASE_URL,
            }
        else:  # OLLAMA
            return {
                "provider": "ollama", 
                "model": cls.OLLAMA_MODEL,
                "base_url": cls.OLLAMA_BASE_URL,
            }
    
    @classmethod
    def is_openai_configured(cls) -> bool:
        """OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return bool(cls.OPENAI_API_KEY)
    
    @classmethod
    def is_streamlit_cloud(cls) -> bool:
        """Streamlit Cloud í™˜ê²½ì¸ì§€ í™•ì¸"""
        import sys
        
        # Streamlit Cloud ê°ì§€ ì¡°ê±´ë“¤
        cloud_indicators = [
            "/app/" in str(Path.cwd()),                    # Streamlit Cloud ê¸°ë³¸ ê²½ë¡œ
            "/mount/src/" in str(Path.cwd()),              # GitHub ì—°ë™ ê²½ë¡œ
            "STREAMLIT_CLOUD" in os.environ,               # í™˜ê²½ ë³€ìˆ˜
            "STREAMLIT" in os.environ,                     # ëŒ€ì•ˆ í™˜ê²½ ë³€ìˆ˜
            hasattr(sys, 'ps1') is False,                  # ë¹„ëŒ€í™”í˜• í™˜ê²½
            "streamlit" in str(sys.executable).lower(),    # Streamlit ì‹¤í–‰ í™˜ê²½
        ]
        
        is_cloud = any(cloud_indicators)
        print(f"ğŸ” Streamlit Cloud ê°ì§€ ê²°ê³¼: {is_cloud}")
        print(f"   - í˜„ì¬ ê²½ë¡œ: {Path.cwd()}")
        print(f"   - Python ì‹¤í–‰íŒŒì¼: {sys.executable}")
        print(f"   - í™˜ê²½ ë³€ìˆ˜ STREAMLIT: {'STREAMLIT' in os.environ}")
        
        return is_cloud
    
    @classmethod
    def ensure_directories(cls):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        cls.MODELS_DIR.mkdir(exist_ok=True)
        cls.TEMP_DIR.mkdir(exist_ok=True)